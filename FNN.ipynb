{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3900bce3-8668-4fe9-a262-041ee436be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdae6dc-ff9d-4901-8dbd-68168b06b25c",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6f3c937-9ee6-482e-a467-2a0a226644f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "591ccd41-31c4-4bda-a3b8-98ab63324695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions\n",
    "#variables which are the result of a PCA transformation\n",
    "#'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset.\n",
    "#'Amount' is the transaction Amount\n",
    "# Class == 0 : normal, Class == 1 : anomaly\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92db98a6-21c0-40c0-9027-3ff2d419e1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3f0b552-84e4-4844-a509-dab39f01e828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>406.0</td>\n",
       "      <td>-2.312227</td>\n",
       "      <td>1.951992</td>\n",
       "      <td>-1.609851</td>\n",
       "      <td>3.997906</td>\n",
       "      <td>-0.522188</td>\n",
       "      <td>-1.426545</td>\n",
       "      <td>-2.537387</td>\n",
       "      <td>1.391657</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517232</td>\n",
       "      <td>-0.035049</td>\n",
       "      <td>-0.465211</td>\n",
       "      <td>0.320198</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.177840</td>\n",
       "      <td>0.261145</td>\n",
       "      <td>-0.143276</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>472.0</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>4462.0</td>\n",
       "      <td>-2.303350</td>\n",
       "      <td>1.759247</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>2.330243</td>\n",
       "      <td>-0.821628</td>\n",
       "      <td>-0.075788</td>\n",
       "      <td>0.562320</td>\n",
       "      <td>-0.399147</td>\n",
       "      <td>-0.238253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294166</td>\n",
       "      <td>-0.932391</td>\n",
       "      <td>0.172726</td>\n",
       "      <td>-0.087330</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>-0.542628</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>-0.153029</td>\n",
       "      <td>239.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>6986.0</td>\n",
       "      <td>-4.397974</td>\n",
       "      <td>1.358367</td>\n",
       "      <td>-2.592844</td>\n",
       "      <td>2.679787</td>\n",
       "      <td>-1.128131</td>\n",
       "      <td>-1.706536</td>\n",
       "      <td>-3.496197</td>\n",
       "      <td>-0.248778</td>\n",
       "      <td>-0.247768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573574</td>\n",
       "      <td>0.176968</td>\n",
       "      <td>-0.436207</td>\n",
       "      <td>-0.053502</td>\n",
       "      <td>0.252405</td>\n",
       "      <td>-0.657488</td>\n",
       "      <td>-0.827136</td>\n",
       "      <td>0.849573</td>\n",
       "      <td>59.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6329</th>\n",
       "      <td>7519.0</td>\n",
       "      <td>1.234235</td>\n",
       "      <td>3.019740</td>\n",
       "      <td>-4.304597</td>\n",
       "      <td>4.732795</td>\n",
       "      <td>3.624201</td>\n",
       "      <td>-1.357746</td>\n",
       "      <td>1.713445</td>\n",
       "      <td>-0.496358</td>\n",
       "      <td>-1.282858</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379068</td>\n",
       "      <td>-0.704181</td>\n",
       "      <td>-0.656805</td>\n",
       "      <td>-1.632653</td>\n",
       "      <td>1.488901</td>\n",
       "      <td>0.566797</td>\n",
       "      <td>-0.010016</td>\n",
       "      <td>0.146793</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279863</th>\n",
       "      <td>169142.0</td>\n",
       "      <td>-1.927883</td>\n",
       "      <td>1.125653</td>\n",
       "      <td>-4.518331</td>\n",
       "      <td>1.749293</td>\n",
       "      <td>-1.566487</td>\n",
       "      <td>-2.010494</td>\n",
       "      <td>-0.882850</td>\n",
       "      <td>0.697211</td>\n",
       "      <td>-2.064945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778584</td>\n",
       "      <td>-0.319189</td>\n",
       "      <td>0.639419</td>\n",
       "      <td>-0.294885</td>\n",
       "      <td>0.537503</td>\n",
       "      <td>0.788395</td>\n",
       "      <td>0.292680</td>\n",
       "      <td>0.147968</td>\n",
       "      <td>390.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280143</th>\n",
       "      <td>169347.0</td>\n",
       "      <td>1.378559</td>\n",
       "      <td>1.289381</td>\n",
       "      <td>-5.004247</td>\n",
       "      <td>1.411850</td>\n",
       "      <td>0.442581</td>\n",
       "      <td>-1.326536</td>\n",
       "      <td>-1.413170</td>\n",
       "      <td>0.248525</td>\n",
       "      <td>-1.127396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370612</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>-0.145640</td>\n",
       "      <td>-0.081049</td>\n",
       "      <td>0.521875</td>\n",
       "      <td>0.739467</td>\n",
       "      <td>0.389152</td>\n",
       "      <td>0.186637</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280149</th>\n",
       "      <td>169351.0</td>\n",
       "      <td>-0.676143</td>\n",
       "      <td>1.126366</td>\n",
       "      <td>-2.213700</td>\n",
       "      <td>0.468308</td>\n",
       "      <td>-1.120541</td>\n",
       "      <td>-0.003346</td>\n",
       "      <td>-2.234739</td>\n",
       "      <td>1.210158</td>\n",
       "      <td>-0.652250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751826</td>\n",
       "      <td>0.834108</td>\n",
       "      <td>0.190944</td>\n",
       "      <td>0.032070</td>\n",
       "      <td>-0.739695</td>\n",
       "      <td>0.471111</td>\n",
       "      <td>0.385107</td>\n",
       "      <td>0.194361</td>\n",
       "      <td>77.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281144</th>\n",
       "      <td>169966.0</td>\n",
       "      <td>-3.113832</td>\n",
       "      <td>0.585864</td>\n",
       "      <td>-5.399730</td>\n",
       "      <td>1.817092</td>\n",
       "      <td>-0.840618</td>\n",
       "      <td>-2.943548</td>\n",
       "      <td>-2.208002</td>\n",
       "      <td>1.058733</td>\n",
       "      <td>-1.632333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583276</td>\n",
       "      <td>-0.269209</td>\n",
       "      <td>-0.456108</td>\n",
       "      <td>-0.183659</td>\n",
       "      <td>-0.328168</td>\n",
       "      <td>0.606116</td>\n",
       "      <td>0.884876</td>\n",
       "      <td>-0.253700</td>\n",
       "      <td>245.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281674</th>\n",
       "      <td>170348.0</td>\n",
       "      <td>1.991976</td>\n",
       "      <td>0.158476</td>\n",
       "      <td>-2.583441</td>\n",
       "      <td>0.408670</td>\n",
       "      <td>1.151147</td>\n",
       "      <td>-0.096695</td>\n",
       "      <td>0.223050</td>\n",
       "      <td>-0.068384</td>\n",
       "      <td>0.577829</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164350</td>\n",
       "      <td>-0.295135</td>\n",
       "      <td>-0.072173</td>\n",
       "      <td>-0.450261</td>\n",
       "      <td>0.313267</td>\n",
       "      <td>-0.289617</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.015309</td>\n",
       "      <td>42.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>492 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "541        406.0 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
       "623        472.0 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "4920      4462.0 -2.303350  1.759247 -0.359745  2.330243 -0.821628 -0.075788   \n",
       "6108      6986.0 -4.397974  1.358367 -2.592844  2.679787 -1.128131 -1.706536   \n",
       "6329      7519.0  1.234235  3.019740 -4.304597  4.732795  3.624201 -1.357746   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "279863  169142.0 -1.927883  1.125653 -4.518331  1.749293 -1.566487 -2.010494   \n",
       "280143  169347.0  1.378559  1.289381 -5.004247  1.411850  0.442581 -1.326536   \n",
       "280149  169351.0 -0.676143  1.126366 -2.213700  0.468308 -1.120541 -0.003346   \n",
       "281144  169966.0 -3.113832  0.585864 -5.399730  1.817092 -0.840618 -2.943548   \n",
       "281674  170348.0  1.991976  0.158476 -2.583441  0.408670  1.151147 -0.096695   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "541    -2.537387  1.391657 -2.770089  ...  0.517232 -0.035049 -0.465211   \n",
       "623     0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
       "4920    0.562320 -0.399147 -0.238253  ... -0.294166 -0.932391  0.172726   \n",
       "6108   -3.496197 -0.248778 -0.247768  ...  0.573574  0.176968 -0.436207   \n",
       "6329    1.713445 -0.496358 -1.282858  ... -0.379068 -0.704181 -0.656805   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "279863 -0.882850  0.697211 -2.064945  ...  0.778584 -0.319189  0.639419   \n",
       "280143 -1.413170  0.248525 -1.127396  ...  0.370612  0.028234 -0.145640   \n",
       "280149 -2.234739  1.210158 -0.652250  ...  0.751826  0.834108  0.190944   \n",
       "281144 -2.208002  1.058733 -1.632333  ...  0.583276 -0.269209 -0.456108   \n",
       "281674  0.223050 -0.068384  0.577829  ... -0.164350 -0.295135 -0.072173   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "541     0.320198  0.044519  0.177840  0.261145 -0.143276    0.00      1  \n",
       "623    -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
       "4920   -0.087330 -0.156114 -0.542628  0.039566 -0.153029  239.93      1  \n",
       "6108   -0.053502  0.252405 -0.657488 -0.827136  0.849573   59.00      1  \n",
       "6329   -1.632653  1.488901  0.566797 -0.010016  0.146793    1.00      1  \n",
       "...          ...       ...       ...       ...       ...     ...    ...  \n",
       "279863 -0.294885  0.537503  0.788395  0.292680  0.147968  390.00      1  \n",
       "280143 -0.081049  0.521875  0.739467  0.389152  0.186637    0.76      1  \n",
       "280149  0.032070 -0.739695  0.471111  0.385107  0.194361   77.89      1  \n",
       "281144 -0.183659 -0.328168  0.606116  0.884876 -0.253700  245.00      1  \n",
       "281674 -0.450261  0.313267 -0.289617  0.002988 -0.015309   42.53      1  \n",
       "\n",
       "[492 rows x 31 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.Class.value_counts())\n",
    "data.loc[data.Class==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ac695ab-2bad-4f04-b864-974aebce5057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87a3ae1b-6530-4138-97bc-760637186cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Time'] = data['Time']%86400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4815c6eb-9444-4d05-89d4-25764bf09186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DBSCAN\n",
    "# 2. isolation forest\n",
    "# 3. Gaussian \n",
    "\n",
    "# + SMOTE\n",
    "\n",
    "# https://www.kaggle.com/code/nareshbhat/outlier-the-silent-killer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de8947bd-ca92-4264-bd22-d105df39619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.iloc[:,:-1]\n",
    "labels = data.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3e7c039-170e-4f4f-bad8-e5ddda879579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/valid/test = 6/2/2\n",
    "train_index = int(len(data) * 0.6)\n",
    "valid_index = int(len(data) * 0.8)\n",
    "X_train, y_train = features.iloc[:train_index,:], labels.iloc[:train_index,:]\n",
    "X_val, y_val = features.iloc[train_index:valid_index,:], labels.iloc[train_index:valid_index,:]\n",
    "X_test, y_test = features.iloc[valid_index:,:], labels.iloc[valid_index:,:]\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train.values).float()\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "y_val = torch.from_numpy(y_val.values).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bd31c4b-3fea-4037-a87f-c90c35b31509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data_utils.TensorDataset(X_train, y_train)\n",
    "valid = data_utils.TensorDataset(X_val, y_val)\n",
    "test = data_utils.TensorDataset(X_test, y_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train,batch_size = 256, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid,batch_size = 256, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test,batch_size = 256, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2173bc9f-a94a-4544-af13-20fca8173efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 30])\n",
      "torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(trainloader)\n",
    "features, labels = data_iter.next()\n",
    "\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293419e5-b76a-4fbf-b9a9-07e38573fe70",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "993f2472-3932-47f8-aa70-d67e2d408887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가지표 : accuracy\n",
    "def accuracy(y_pred, y_test) : \n",
    "    prediction = y_pred >= torch.FloatTensor([0.5])\n",
    "    correct_prediction = prediction.float() == y_test\n",
    "    accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00005b5b-25fa-40c2-86fc-0b54daaab815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FNN default 모델 구현\n",
    "class DefaultModel(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(DefaultModel, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(30,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        # x = x.view(x.size(0),-1)\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f6681a7-b823-49b9-8159-d929434ecb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_loss : 0.02555, val_loss : 0.00300, train_accuracy : 0.99420, val_accuracy : 0.99963\n",
      "Epoch: 2, train_loss : 0.00371, val_loss : 0.00249, train_accuracy : 0.99926, val_accuracy : 0.99953\n",
      "Epoch: 3, train_loss : 0.00361, val_loss : 0.00240, train_accuracy : 0.99918, val_accuracy : 0.99968\n",
      "Epoch: 4, train_loss : 0.00345, val_loss : 0.00292, train_accuracy : 0.99923, val_accuracy : 0.99968\n",
      "Epoch: 5, train_loss : 0.00335, val_loss : 0.00643, train_accuracy : 0.99926, val_accuracy : 0.99947\n",
      "Epoch: 6, train_loss : 0.00319, val_loss : 0.00245, train_accuracy : 0.99925, val_accuracy : 0.99967\n",
      "Epoch: 7, train_loss : 0.00302, val_loss : 0.00291, train_accuracy : 0.99930, val_accuracy : 0.99961\n",
      "Epoch: 8, train_loss : 0.00299, val_loss : 0.00258, train_accuracy : 0.99930, val_accuracy : 0.99967\n",
      "Epoch: 9, train_loss : 0.00280, val_loss : 0.00294, train_accuracy : 0.99935, val_accuracy : 0.99961\n",
      "Epoch: 10, train_loss : 0.00258, val_loss : 0.01687, train_accuracy : 0.99936, val_accuracy : 0.99471\n",
      "Epoch: 11, train_loss : 0.00253, val_loss : 0.00266, train_accuracy : 0.99935, val_accuracy : 0.99961\n",
      "Epoch: 12, train_loss : 0.00232, val_loss : 0.00442, train_accuracy : 0.99934, val_accuracy : 0.99905\n",
      "Epoch: 13, train_loss : 0.00221, val_loss : 0.00264, train_accuracy : 0.99942, val_accuracy : 0.99967\n",
      "Epoch: 14, train_loss : 0.00197, val_loss : 0.00300, train_accuracy : 0.99940, val_accuracy : 0.99963\n",
      "Epoch: 15, train_loss : 0.00203, val_loss : 0.00321, train_accuracy : 0.99934, val_accuracy : 0.99951\n",
      "Epoch: 16, train_loss : 0.00191, val_loss : 0.00304, train_accuracy : 0.99945, val_accuracy : 0.99963\n",
      "Epoch: 17, train_loss : 0.00182, val_loss : 0.00798, train_accuracy : 0.99946, val_accuracy : 0.99758\n",
      "Epoch: 18, train_loss : 0.00176, val_loss : 0.00351, train_accuracy : 0.99948, val_accuracy : 0.99949\n",
      "Epoch: 19, train_loss : 0.00162, val_loss : 0.00329, train_accuracy : 0.99952, val_accuracy : 0.99961\n",
      "Epoch: 20, train_loss : 0.00140, val_loss : 0.00412, train_accuracy : 0.99957, val_accuracy : 0.99963\n",
      "Epoch: 21, train_loss : 0.00157, val_loss : 0.00341, train_accuracy : 0.99953, val_accuracy : 0.99960\n",
      "Epoch: 22, train_loss : 0.00159, val_loss : 0.00367, train_accuracy : 0.99953, val_accuracy : 0.99947\n",
      "Epoch: 23, train_loss : 0.00138, val_loss : 0.00397, train_accuracy : 0.99954, val_accuracy : 0.99956\n",
      "Epoch: 24, train_loss : 0.00136, val_loss : 0.00462, train_accuracy : 0.99956, val_accuracy : 0.99965\n",
      "Epoch: 25, train_loss : 0.00117, val_loss : 0.00557, train_accuracy : 0.99957, val_accuracy : 0.99907\n",
      "Epoch: 26, train_loss : 0.00115, val_loss : 0.00449, train_accuracy : 0.99964, val_accuracy : 0.99967\n",
      "Epoch: 27, train_loss : 0.00109, val_loss : 0.00476, train_accuracy : 0.99965, val_accuracy : 0.99961\n",
      "Epoch: 28, train_loss : 0.00113, val_loss : 0.00413, train_accuracy : 0.99961, val_accuracy : 0.99963\n",
      "Epoch: 29, train_loss : 0.00103, val_loss : 0.00393, train_accuracy : 0.99971, val_accuracy : 0.99960\n",
      "Epoch: 30, train_loss : 0.00110, val_loss : 0.00455, train_accuracy : 0.99964, val_accuracy : 0.99946\n",
      "Epoch: 31, train_loss : 0.00090, val_loss : 0.00475, train_accuracy : 0.99972, val_accuracy : 0.99958\n",
      "Epoch: 32, train_loss : 0.00112, val_loss : 0.01298, train_accuracy : 0.99965, val_accuracy : 0.99601\n",
      "Epoch: 33, train_loss : 0.00089, val_loss : 0.00503, train_accuracy : 0.99970, val_accuracy : 0.99953\n",
      "Epoch: 34, train_loss : 0.00095, val_loss : 0.00522, train_accuracy : 0.99966, val_accuracy : 0.99919\n",
      "Epoch: 35, train_loss : 0.00085, val_loss : 0.00467, train_accuracy : 0.99977, val_accuracy : 0.99956\n",
      "Epoch: 36, train_loss : 0.00087, val_loss : 0.00452, train_accuracy : 0.99973, val_accuracy : 0.99961\n",
      "Epoch: 37, train_loss : 0.00084, val_loss : 0.00493, train_accuracy : 0.99967, val_accuracy : 0.99953\n",
      "Epoch: 38, train_loss : 0.00080, val_loss : 0.00496, train_accuracy : 0.99973, val_accuracy : 0.99965\n",
      "Epoch: 39, train_loss : 0.00074, val_loss : 0.06295, train_accuracy : 0.99976, val_accuracy : 0.97523\n",
      "Epoch: 40, train_loss : 0.00085, val_loss : 0.00569, train_accuracy : 0.99973, val_accuracy : 0.99916\n",
      "Epoch: 41, train_loss : 0.00069, val_loss : 0.00588, train_accuracy : 0.99976, val_accuracy : 0.99958\n",
      "Epoch: 42, train_loss : 0.00078, val_loss : 0.00576, train_accuracy : 0.99974, val_accuracy : 0.99953\n",
      "Epoch: 43, train_loss : 0.00079, val_loss : 0.00496, train_accuracy : 0.99976, val_accuracy : 0.99954\n",
      "Epoch: 44, train_loss : 0.00057, val_loss : 0.00545, train_accuracy : 0.99981, val_accuracy : 0.99951\n",
      "Epoch: 45, train_loss : 0.00077, val_loss : 0.00528, train_accuracy : 0.99975, val_accuracy : 0.99933\n",
      "Epoch: 46, train_loss : 0.00075, val_loss : 0.00552, train_accuracy : 0.99975, val_accuracy : 0.99928\n",
      "Epoch: 47, train_loss : 0.00050, val_loss : 0.00576, train_accuracy : 0.99981, val_accuracy : 0.99928\n",
      "Epoch: 48, train_loss : 0.00086, val_loss : 0.00480, train_accuracy : 0.99976, val_accuracy : 0.99961\n",
      "Epoch: 49, train_loss : 0.00053, val_loss : 0.00627, train_accuracy : 0.99982, val_accuracy : 0.99942\n",
      "Epoch: 50, train_loss : 0.00066, val_loss : 0.00475, train_accuracy : 0.99977, val_accuracy : 0.99956\n",
      "Epoch: 51, train_loss : 0.00076, val_loss : 0.00511, train_accuracy : 0.99977, val_accuracy : 0.99954\n",
      "Epoch: 52, train_loss : 0.00064, val_loss : 0.00602, train_accuracy : 0.99980, val_accuracy : 0.99940\n",
      "Epoch: 53, train_loss : 0.00055, val_loss : 0.00581, train_accuracy : 0.99980, val_accuracy : 0.99956\n",
      "Epoch: 54, train_loss : 0.00060, val_loss : 0.00578, train_accuracy : 0.99975, val_accuracy : 0.99960\n",
      "Epoch: 55, train_loss : 0.00059, val_loss : 0.00550, train_accuracy : 0.99982, val_accuracy : 0.99960\n",
      "Epoch: 56, train_loss : 0.00064, val_loss : 0.00550, train_accuracy : 0.99980, val_accuracy : 0.99956\n",
      "Epoch: 57, train_loss : 0.00053, val_loss : 0.00564, train_accuracy : 0.99984, val_accuracy : 0.99963\n",
      "Epoch: 58, train_loss : 0.00067, val_loss : 0.00663, train_accuracy : 0.99980, val_accuracy : 0.99963\n",
      "Epoch: 59, train_loss : 0.00079, val_loss : 0.00522, train_accuracy : 0.99975, val_accuracy : 0.99946\n",
      "Epoch: 60, train_loss : 0.00039, val_loss : 0.00604, train_accuracy : 0.99984, val_accuracy : 0.99947\n",
      "Epoch: 61, train_loss : 0.00063, val_loss : 0.00646, train_accuracy : 0.99978, val_accuracy : 0.99947\n",
      "Epoch: 62, train_loss : 0.00053, val_loss : 0.00586, train_accuracy : 0.99982, val_accuracy : 0.99951\n",
      "Epoch: 63, train_loss : 0.00039, val_loss : 0.00700, train_accuracy : 0.99987, val_accuracy : 0.99951\n",
      "Epoch: 64, train_loss : 0.00044, val_loss : 0.00674, train_accuracy : 0.99986, val_accuracy : 0.99954\n",
      "Epoch: 65, train_loss : 0.00061, val_loss : 0.00671, train_accuracy : 0.99978, val_accuracy : 0.99965\n",
      "Epoch: 66, train_loss : 0.00056, val_loss : 0.00677, train_accuracy : 0.99980, val_accuracy : 0.99940\n",
      "Epoch: 67, train_loss : 0.00055, val_loss : 0.00632, train_accuracy : 0.99984, val_accuracy : 0.99958\n",
      "Epoch: 68, train_loss : 0.00050, val_loss : 0.00624, train_accuracy : 0.99984, val_accuracy : 0.99911\n",
      "Epoch: 69, train_loss : 0.00045, val_loss : 0.00666, train_accuracy : 0.99984, val_accuracy : 0.99960\n",
      "Epoch: 70, train_loss : 0.00045, val_loss : 0.00684, train_accuracy : 0.99984, val_accuracy : 0.99961\n",
      "Epoch: 71, train_loss : 0.00048, val_loss : 0.00672, train_accuracy : 0.99982, val_accuracy : 0.99946\n",
      "Epoch: 72, train_loss : 0.00034, val_loss : 0.00680, train_accuracy : 0.99988, val_accuracy : 0.99951\n",
      "Epoch: 73, train_loss : 0.00056, val_loss : 0.00655, train_accuracy : 0.99982, val_accuracy : 0.99921\n",
      "Epoch: 74, train_loss : 0.00067, val_loss : 0.00497, train_accuracy : 0.99975, val_accuracy : 0.99942\n",
      "Epoch: 75, train_loss : 0.00036, val_loss : 0.00596, train_accuracy : 0.99985, val_accuracy : 0.99958\n",
      "Epoch: 76, train_loss : 0.00055, val_loss : 0.00568, train_accuracy : 0.99978, val_accuracy : 0.99949\n",
      "Epoch: 77, train_loss : 0.00044, val_loss : 0.00571, train_accuracy : 0.99983, val_accuracy : 0.99961\n",
      "Epoch: 78, train_loss : 0.00036, val_loss : 0.00626, train_accuracy : 0.99989, val_accuracy : 0.99949\n",
      "Epoch: 79, train_loss : 0.00041, val_loss : 0.00687, train_accuracy : 0.99984, val_accuracy : 0.99951\n",
      "Epoch: 80, train_loss : 0.00050, val_loss : 0.00655, train_accuracy : 0.99984, val_accuracy : 0.99953\n",
      "Epoch: 81, train_loss : 0.00040, val_loss : 0.00644, train_accuracy : 0.99986, val_accuracy : 0.99951\n",
      "Epoch: 82, train_loss : 0.00052, val_loss : 0.00619, train_accuracy : 0.99981, val_accuracy : 0.99953\n",
      "Epoch: 83, train_loss : 0.00021, val_loss : 0.00746, train_accuracy : 0.99991, val_accuracy : 0.99926\n",
      "Epoch: 84, train_loss : 0.00057, val_loss : 0.00699, train_accuracy : 0.99984, val_accuracy : 0.99946\n",
      "Epoch: 85, train_loss : 0.00052, val_loss : 0.00554, train_accuracy : 0.99980, val_accuracy : 0.99942\n",
      "Epoch: 86, train_loss : 0.00037, val_loss : 0.00640, train_accuracy : 0.99989, val_accuracy : 0.99937\n",
      "Epoch: 87, train_loss : 0.00035, val_loss : 0.00611, train_accuracy : 0.99988, val_accuracy : 0.99930\n",
      "Epoch: 88, train_loss : 0.00062, val_loss : 0.00605, train_accuracy : 0.99980, val_accuracy : 0.99953\n",
      "Epoch: 89, train_loss : 0.00020, val_loss : 0.00685, train_accuracy : 0.99995, val_accuracy : 0.99949\n",
      "Epoch: 90, train_loss : 0.00037, val_loss : 0.00710, train_accuracy : 0.99984, val_accuracy : 0.99958\n",
      "Epoch: 91, train_loss : 0.00063, val_loss : 0.00605, train_accuracy : 0.99976, val_accuracy : 0.99961\n",
      "Epoch: 92, train_loss : 0.00033, val_loss : 0.00619, train_accuracy : 0.99989, val_accuracy : 0.99965\n",
      "Epoch: 93, train_loss : 0.00026, val_loss : 0.00736, train_accuracy : 0.99994, val_accuracy : 0.99960\n",
      "Epoch: 94, train_loss : 0.00036, val_loss : 0.00696, train_accuracy : 0.99989, val_accuracy : 0.99965\n",
      "Epoch: 95, train_loss : 0.00052, val_loss : 0.00639, train_accuracy : 0.99980, val_accuracy : 0.99956\n",
      "Epoch: 96, train_loss : 0.00046, val_loss : 0.00615, train_accuracy : 0.99984, val_accuracy : 0.99963\n",
      "Epoch: 97, train_loss : 0.00035, val_loss : 0.00582, train_accuracy : 0.99988, val_accuracy : 0.99956\n",
      "Epoch: 98, train_loss : 0.00027, val_loss : 0.00727, train_accuracy : 0.99991, val_accuracy : 0.99967\n",
      "Epoch: 99, train_loss : 0.00040, val_loss : 0.00653, train_accuracy : 0.99986, val_accuracy : 0.99954\n",
      "Epoch: 100, train_loss : 0.00051, val_loss : 0.00618, train_accuracy : 0.99983, val_accuracy : 0.99947\n"
     ]
    }
   ],
   "source": [
    "# FNN-defalut 학습/평가\n",
    "loss_fn = nn.BCELoss()\n",
    "model = DefaultModel()#.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "loss_train_df = []\n",
    "accuracy_train_df = []\n",
    "loss_val_df = []\n",
    "accuracy_val_df = []\n",
    "y_val_list_df = []\n",
    "y_pred_list_df = []\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    for x_train, y_train in trainloader :\n",
    "        # x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train)\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy(y_pred, y_train)\n",
    "        \n",
    "    model.eval() \n",
    "    with torch.no_grad() :\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x_val, y_val in validloader :\n",
    "            # x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "            y_pred = model(x_val)\n",
    "            val_loss += loss_fn(y_pred, y_val)\n",
    "            val_acc += accuracy(y_pred, y_val)\n",
    "            # if epoch == 19 :\n",
    "            #     y_val_list_df.append(y_val.tolist())\n",
    "            #     y_pred_max = torch.max(y_val,1)\n",
    "            #     y_pred_list_df.append(y_pred_max.indices.tolist())\n",
    "            if epoch == 19 :\n",
    "                y_val_list_df.append(y_val.tolist())\n",
    "                # y_pred_max = torch.max(y_val,1)\n",
    "                y_pred = [1 if i >0.5 else 0 for a in y_pred.tolist() for i in a]\n",
    "                y_pred_list_df.append(y_pred)\n",
    "    \n",
    "    loss_train_df.append(train_loss/len(trainloader))\n",
    "    loss_val_df.append(val_loss/len(validloader))\n",
    "    accuracy_train_df.append(train_acc/len(trainloader))\n",
    "    accuracy_val_df.append(val_acc/len(validloader))\n",
    "   \n",
    "    \n",
    "    print('Epoch: {}, train_loss : {:.5f}, val_loss : {:.5f}, train_accuracy : {:.5f}, val_accuracy : {:.5f}'\\\n",
    "        .format(epoch+1,train_loss/len(trainloader),val_loss/len(validloader), train_acc/len(trainloader), val_acc/len(validloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3fda5f-2279-45c1-9774-121d0e45897e",
   "metadata": {},
   "source": [
    "### training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ddec128-a538-4935-8e3c-232d6fb6573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형변환(cuda -> numpy)\n",
    "def cuda_numpy(data_cuda) :\n",
    "    data_numpy = [i.cpu().detach().numpy() for i in data_cuda]\n",
    "    return data_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3924e2c7-1969-432d-99a0-7d3e3e2cb462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'train_loss')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(cuda_numpy(loss_train_lr), label='logistic')\n",
    "plt.plot(cuda_numpy(loss_train_df), label='default')\n",
    "# plt.plot(cuda_numpy(loss_train_bn), label='batch_norm')\n",
    "# plt.plot(cuda_numpy(loss_train_dr), label='dropout')\n",
    "plt.legend()\n",
    "plt.title('train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8740f9a4-5aa4-49bd-afb0-24ffcc9eaf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56899,     5],\n",
       "       [   16,    41]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FNN-default confusion matrix 확인\n",
    "# x = np.array(y_pred_list_df).flatten()\n",
    "# y = np.array(y_val_list_df).flatten()\n",
    "# confusion_matrix(x,y)\n",
    "# confusion_matrix_df = pd.DataFrame(confusion_matrix(x,y)/np.sum(confusion_matrix(x,y)))#,index = [0 ,1], columns = [0,1])\n",
    "# plt.figure(figsize = (12,10))\n",
    "# plt.title('default')\n",
    "# sns.heatmap(confusion_matrix_df, annot=True)\n",
    "# x = y_pred_list_df\n",
    "# y = y_val_list_df\n",
    "# confusion_matrix(x,y)\n",
    "# f y_pred_list_df\n",
    "# x.flatten()\n",
    "x = [a for b in y_val_list_df for a in b]\n",
    "y = [a for b in y_pred_list_df for a in b]\n",
    "confusion_matrix(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8854d5a-d61f-401a-ae7d-c996eb18f845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     56904\n",
      "         1.0       0.89      0.72      0.80        57\n",
      "\n",
      "    accuracy                           1.00     56961\n",
      "   macro avg       0.95      0.86      0.90     56961\n",
      "weighted avg       1.00      1.00      1.00     56961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677f2f2-dd46-402f-8e5e-afe09ac962e6",
   "metadata": {},
   "source": [
    "# test set 평가 : confusion matrix, f1-score, PR-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b054281-1e38-4557-ba88-f7b887cf4457",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "test_list = []\n",
    "pred_list = []\n",
    "prob_list = []\n",
    "\n",
    "with torch.no_grad() :\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x_test, y_test in testloader :\n",
    "        # x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "        y_pred = model(x_test)\n",
    "        test_list.append(y_test.tolist())\n",
    "        # y_pred_max = torch.max(y_pred,1)\n",
    "        prob_list.append(y_pred)\n",
    "        y_pred = [1 if i >0.5 else 0 for a in y_pred.tolist() for i in a]\n",
    "        pred_list.append(y_pred)\n",
    "        # print(pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75979bd6-542e-429b-b533-c4fd67f31299",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6a9a38d-9f5b-4973-bcd0-f12ad775393a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56848,    39],\n",
       "       [   16,    59]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ = [a for b in test_list for a in b]\n",
    "y_ = [a for b in pred_list for a in b]\n",
    "y_prob = [a for b in prob_list for a in b]\n",
    "confusion_matrix(x_,y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dad1a4a-1041-4a18-9930-2f36096e6872",
   "metadata": {},
   "source": [
    "### f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33d42366-547e-4c6c-a287-9f51ecdb96a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     56887\n",
      "         1.0       0.60      0.79      0.68        75\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.80      0.89      0.84     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(x_, y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a7087-fce4-4806-80c7-e692722db1bb",
   "metadata": {},
   "source": [
    "### PR-AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df8c18a0-14ae-449f-88f3-2d19c2695c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score \n",
    "def pr_auc(target, losses):\n",
    "\n",
    "    precision, recalls, threshold = precision_recall_curve(target, losses) # _ = threshold\n",
    "    average_precision = average_precision_score(target, losses)\n",
    "    # print(precision)\n",
    "    # print(recalls)\n",
    "    # print(threshold)\n",
    "    print(\"Average precision-recall score: {0:.4f} \".format(average_precision))\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(recalls, precision, label=\"precision & recall\")\n",
    "    plt.xlabel(\"recall\")\n",
    "    plt.ylabel(\"precision\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01132d72-6644-4caf-bbdf-a2cc8ad78008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.7810 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pr_auc(x_,y_prob)\n",
    "# loss_val_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
